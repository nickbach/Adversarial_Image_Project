Report Abstract:
By studying adversarial images and how they are made we gain a much deeper understanding of how deep learning algorithms work. I assesed the adversarial methods, (1) fast gradient sign method (FGSM), (2) iterative least-likely class method (ILCM), and (3) a novel method, through three facets: visual analysis of FGSM, how effective the novel method is at reducing confidence levels, and fooling rate. Each analysis gave different insights into how each method works and how effective each method is. Though adversarial images are a threat to deep learning software, with more research into the topic it is likely that software will become more robust and defend against adversarial methods more successfully.
Project Proposal:
	In the past few years I have read and been terrified by articles about facial recognition software. According to the New York Times, https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html, China has used facial recognition software to surveil and target Uighurs, and Last Week Tonight put out a piece on how facial recognition has been used by our government domestically: https://www.youtube.com/watch?v=jZjmlJPJgug. When I first watched that segment, I tried to think of a way to combat this and similar software. What I pictured was flooding the system with images that looked nothing like a certain individual to humans but caused the software to recognize it as that individual. I didn’t know the name for it at the time, but this falls under an umbrella of adversarial image attacks, which is the topic I would like to read reports on and try to implement for this project.
	There are many different facets of adversarial image attacks to explore, and so I read many articles about different ways it can be implemented, compare how and why each implementation works, and then implement my own version of one. As a starting point, I will read “What Machines See Is Not What They Get: Fooling Scene Text Recognition Models With Adversarial Text Images” from CVPR 2020 about fooling text recognition software. “Adversarial Fooling Beyond ‘Flipping the Label’” from CVPR 2020 will help me explore terminology around measuring the efficacy of a software at preventing adversarial attacks. “Adversarial Attacks on Convolutional Neural Networks in Facial Recognition Domain” from Cornell discusses the effectiveness of single-pixel and all-pixel attacks on facial recognition software, and “Defending Black Box Facial Recognition Classifiers Against Adversarial Attacks” again from CVPR 2020 discusses defending facial recognition software against such attacks.
