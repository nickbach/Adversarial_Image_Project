"# Adversarial_Image_Project"
Report exploring different adversarial attacks in report.pdf.
Project Report:
In the past few years I have read and been terrified by articles about facial recognition software. According to the New York Times, https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html, China has used facial recognition software to surveil and target Uighurs, and Last Week Tonight put out a piece on how facial recognition has been used by our government domestically: https://www.youtube.com/watch?v=jZjmlJPJgug. When I first watched that segment, I tried to think of a way to combat this and similar software. What I pictured was flooding the system with images that looked nothing like a certain individual to humans but caused the software to recognize it as that individual. I didn’t know the name for it at the time, but this falls under an umbrella of adversarial image attacks, which is the topic I would like to read reports on and try to implement for this project.
	There are many different facets of adversarial image attacks to explore, and so I read many articles about different ways it can be implemented, compare how and why each implementation works, and then implement my own version of one. As a starting point, I will read “What Machines See Is Not What They Get: Fooling Scene Text Recognition Models With Adversarial Text Images” from CVPR 2020 about fooling text recognition software. “Adversarial Fooling Beyond ‘Flipping the Label’” from CVPR 2020 will help me explore terminology around measuring the efficacy of a software at preventing adversarial attacks. “Adversarial Attacks on Convolutional Neural Networks in Facial Recognition Domain” from Cornell discusses the effectiveness of single-pixel and all-pixel attacks on facial recognition software, and “Defending Black Box Facial Recognition Classifiers Against Adversarial Attacks” again from CVPR 2020 discusses defending facial recognition software against such attacks.
	I am using these ideas as a starting point, but I expect my project to at least slightly change as I read more about adversarial attacks on recognition software. “Beyond Digital Domain: Fooling Deep Learning Based Recognition System in Physical World” from AAAI Conference 2020, for instance, discusses fooling a DNN computer vision model by building physical objects that it couldn’t recognize correctly. I had only thought about these attacks from a computational standpoint, but this attack used real world objects, which is very interesting. I think I could also explore defenses against such attacks instead of the attacks themselves. I plan on condensing these many ideas into a strong cohesive report, and I’m excited to begin researching.
